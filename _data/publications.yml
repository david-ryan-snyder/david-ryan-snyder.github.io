- paper_title: "Diarization is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge"
  author_list: Gregory Sell, David Snyder, Alan McCree, Daniel Garcia-Romero, Jesus Villalba, Matthew Maciejewski, Vimal Manohar, Najim Dehak, Daniel Povey, Shinji Watanabe and Sanjeev Khudanpur
  journal: Interspeech 2018
  abstract_text: We describe in this paper the experiences of the Johns Hopkins University team during the inaugural DIHARD diarization evaluation. This new task provided microphone recordings in a variety of difficult conditions and challenged researchers to fully consider all speaker activity, without the currently typical practices of unscored collars or ignored overlapping speaker segments. This paper includes details about research decisions made in the process of the evaluation, including explorations of training data, feature bandwidth, speech activity detection, and i-vector versus x-vector representations. The utility of system fusions and domain-specific processing are also considered. In all cases, deeper analyses are presented using the development data, for which truth labels were given during the evaluation, while evaluation scores are only presented for a subset of the systems. Finally, we discuss lessons learned and remaining challenges left for future work within the lens of this new approach to diarization performance measurement.
  pdf_link: http://www.danielpovey.com/files/2018_interspeech_dihard.pdf
  code_link:
  slides_link:
  poster_link:
  video_link:

- paper_title: "Fast variational Bayes for heavy-tailed PLDA applied to i-vectors and x-vectors"
  author_list: Anna Silnova, Niko Brummer, Daniel Garcia-Romero, David Snyder, Lukas Burget
  journal: Interspeech 2018
  abstract_text: The standard state-of-the-art backend for text-independent speaker recognizers that use i-vectors or x-vectors, is Gaussian PLDA (G-PLDA), assisted by a Gaussianization step involving length normalization. G-PLDA can be trained with both generative or discriminative methods. It has long been known that heavy-tailed PLDA (HT-PLDA), applied without length normalization, gives similar accuracy, but at considerable extra computational cost. We have recently introduced a fast scoring algorithm for a discriminatively trained HT-PLDA backend. This paper extends that work by introducing a fast, variational Bayes, generative training algorithm. We compare old and new backends, with and without length-normalization, with i-vectors and x-vectors, on SRE'10, SRE'16 and SITW.
  pdf_link: https://arxiv.org/pdf/1803.09153.pdf
  code_link: https://github.com/bsxfan/meta-embeddings/tree/master/code/Niko/matlab/clean/VB4HTPLDA
  slides_link:
  poster_link:
  video_link:

- paper_title: "Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification"
  author_list: Yingke Zhu, Tom Ko, David Snyder, Brian Mak and Dan Povey
  journal: Interspeech 2018
  abstract_text:
  pdf_link:
  code_link:
  slides_link:
  poster_link:
  video_link:

- paper_title: "Language Recognition for Telephone and Video Speech: The JHU HLTCOE Submission for NIST LRE17"
  author_list: Alan McCree, David Snyder, Gregory Sell, and Daniel Garcia-Romero
  journal: Odyssey 2018
  abstract_text: This paper presents our newest language recognition systems developed for NIST LRE17. For this challenging limited data multidomain task, we were able to get very good performance with our state-of-the-art DNN senone and bottleneck joint ivector systems by effective utilization of all of the available training and development data. Data augmentation techniques were very valuable for this task, and our discriminative Gaussian classifier combined with naive fusion used all of the development data for system design rather than holding some out for separate back-end training. Finally, our newest research with discriminatively-trained DNN embeddings allowed us to replace i-vectors with more powerful x-vectors to further improve language recognition accuracy, resulting in very good LRE17 performance for this single system, our JHU HLTCOE site fusion primary submission, and the JHU MIT team submission.
  pdf_link: https://www.isca-speech.org/archive/Odyssey_2018/pdfs/70.pdf
  code_link:
  slides_link:
  poster_link:
  video_link:

- paper_title: "Spoken Language Recognition using X-vectors"
  author_list: David Snyder, Daniel Garcia-Romero, Alan McCree, Gregory Sell, Daniel Povey, Sanjeev Khudanpur 
  journal: Odyssey 2018
  abstract_text: In this paper, we apply x-vectors to the task of spoken language recognition. This framework consists of a deep neural network that maps sequences of speech features to fixed-dimensional embeddings, called x-vectors. Long-term language characteristics are captured in the network by a temporal pooling layer that aggregates information across time. Once extracted, x-vectors utilize the same classification technology developed for i-vectors. In the 2017 NIST language recognition evaluation, x-vectors achieved excellent results and outperformed our state-ofthe-art i-vector systems. In the post-evaluation analysis presented here, we experiment with several variations of the x-vector framework, and find that the best performing system uses multilingual bottleneck features, data augmentation, and a discriminative Gaussian classifier.
  pdf_link: https://www.isca-speech.org/archive/Odyssey_2018/pdfs/38.pdf
  code_link:
  slides_link:
  poster_link:
  video_link:

- paper_title: "X-vectors: Robust DNN Embeddings for Speaker Recognition"
  author_list: David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, Sanjeev Khudanpur
  journal: IEEE ICASSP 2018
  abstract_text: In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.
  pdf_link: http://www.danielpovey.com/files/2018_icassp_xvectors.pdf
  code_link: https://github.com/kaldi-asr/kaldi/tree/master/egs/sre16/v2
  slides_link:
  poster_link:
  video_link:

- paper_title: "Audio-visual Person Recognition In Multimedia Data From The IARPA Janus Program"
  author_list: Gregory Sell, Kevin Duh, David Snyder, Dave Etter, Daniel Garcia-Romero
  journal: IEEE ICASSP 2018
  abstract_text:
  pdf_link:
  code_link:
  slides_link:
  poster_link:
  video_link:

- paper_title: "Characterizing Performance Of Speaker Diarization Systems On Far-field Speech Using Standard Methods"
  author_list: Matthew Maciejewski, David Snyder, Vimal Manohar, Najim Dehak, Sanjeev Khudanpur,
  journal: IEEE ICASSP 2018
  abstract_text:
  pdf_link:
  code_link:
  slides_link:
  poster_link:
  video_link:

- paper_title: Compressed time delay neural network for small-footprint keyword spotting
  author_list: Ming Sun, David Snyder, Yixin Gao, Varun Nagaraja, Mike Rodehorst, Nikko Strom Panchapagesan, Spyros Matsoukas, Shiv Vitaladevuni
  abstract_text: In this paper we investigate a time delay neural network (TDNN) for a keyword spotting task that requires low CPU, memory and latency. The TDNN is trained with transfer learning and multi-task learning. Temporal subsampling enabled by the time delay architecture reduces computational complexity. We propose to apply singular value decomposition (SVD) to further reduce TDNN complexity. This allows us to first train a larger full-rank TDNN model which is not limited by CPU/memory constraints. The larger TDNN usually achieves better performance. Afterwards, its size can be compressed by SVD to meet the budget requirements. Hidden Markov models (HMM) are used in conjunction with the networks to perform keyword detection and performance is measured in terms of area under the curve (AUC) for detection error tradeoff (DET) curves. Our experimental results on a large in-house far-field corpus show that the full-rank TDNN achieves a 19.7% DET AUC reduction compared to a similar-size deep neural network (DNN) baseline. If we train a larger size full-rank TDNN first and then reduce it via SVD to the comparable size of the DNN, we obtain a 37.6% reduction in DET AUC compared to the DNN baseline.
  pdf_link: https://pdfs.semanticscholar.org/8eda/5fa7103406403a14342336c684b666dfdfc8.pdf
  journal: INTERSPEECH 2017
  code_link:
  slides_link:
  poster_link:
  video_link:

- paper_title: Deep Neural Network Embeddings for Text-Independent Speaker Verification
  author_list: David Snyder, Daniel Garcia-Romero, Daniel Povey, Sanjeev Khudanpur
  abstract_text: This paper investigates replacing i-vectors for text-independent speaker verification with embeddings extracted from a feedforward deep neural network. Long-term speaker characteristics are captured in the network by a temporal pooling layer that aggregates over the input speech. This enables the network to be trained to discriminate between speakers from variable-length speech segments. After training, utterances are mapped directly to fixed-dimensional speaker embeddings and pairs of embeddings are scored using a PLDA-based backend. We compare performance with a traditional i-vector baseline on NIST SRE 2010 and 2016. We find that the embeddings outperform i-vectors for short speech segments and are competitive on long duration test conditions. Moreover, the two representations are complementary, and their fusion improves on the baseline at all operating points. Similar systems have recently shown promising results when trained on very large proprietary datasets, but to the best of our knowledge, these are the best results reported for speaker-discriminative neural networks when trained and tested on publicly available corpora.
  journal: INTERSPEECH 2017
  pdf_link: http://www.danielpovey.com/files/2017_interspeech_embeddings.pdf
  code_link: 
  slides_link:
  poster_link:
  video_link:

- paper_title: Speaker Diarization using Deep Neural Network Embeddings
  author_list: Daniel Garcia-Romero, David Snyder, Gregory Sell, Daniel Povey, Alan McCree
  abstract_text: Speaker diarization is an important front-end for many speech technologies in the presence of multiple speakers, but current methods that employ i-vector clustering for short segments of speech are potentially too cumbersome and costly for the front-end role. In this work, we propose an alternative approach for learning representations via deep neural networks to remove the i-vector extraction process from the pipeline entirely. The proposed architecture simultaneously learns a fixed-dimensional embedding for acoustic segments of variable length and a scoring function for measuring the likelihood that the segments originated from the same or different speakers. Through tests on the CALLHOME conversational telephone speech corpus, we demonstrate that, in addition to streamlining the diarization architecture, the proposed system matches or exceeds the performance of state-of-the-art baselines. We also show that, though this approach does not respond as well to unsupervised calibration strategies as previous systems, the incorporation of well-founded speaker priors sufficiently mitigates this shortcoming.
  journal: IEEE ICASSP 2017
  pdf_link: https://pdfs.semanticscholar.org/d3ed/25e32553e8a399bedc1850c90bf12e4e9d27.pdf
  code_link: 
  slides_link:
  poster_link:
  video_link:

- paper_title: "MUSAN: A Music, Speech, and Noise Corpus"
  author_list: David Snyder, Guoguo Chen, Daniel Povey
  abstract_text: This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.
  journal: arXiv 2015
  pdf_link: http://arxiv.org/pdf/1510.08484v1.pdf
  database: http://www.openslr.org/17/
  code_link:
  slides_link:
  poster_link:
  video_link:


- paper_title: TIME DELAY DEEP NEURAL NETWORK-BASED UNIVERSAL BACKGROUND MODELS FOR SPEAKER RECOGNITION
  author_list: David Snyder, Daniel Garcia-Romero, Daniel Povey
  abstract_text: Recently, deep neural networks (DNN) have been incorporated into i-vector-based speaker recognition systems, where they have significantly improved state-of-the-art performance. In these systems, a DNN is used to collect sufficient statistics for i-vector extraction. In this study, the DNN is a recently developed time delay deep neural network (TDNN) that has achieved promising results in LVCSR tasks. We believe that the TDNN-based system achieves the best reported results on SRE10 and it obtains a 50% relative improvement over our GMM baseline in terms of equal error rate (EER). For some applications, the computational cost of a DNN is high. Therefore, we also investigate a lightweight alternative in which a supervised GMM is derived from the TDNN posteriors. This method maintains the speed of the traditional unsupervised-GMM, but achieves a 20% relative improvement in EER.
  journal: IEEE ASRU 2015
  pdf_link: http://danielpovey.com/files/2015_asru_tdnn_ubm.pdf
  code_link: https://github.com/kaldi-asr/kaldi/tree/master/egs/sre10
  slides_link: 
  poster_link:
  video_link:
