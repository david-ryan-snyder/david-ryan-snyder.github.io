<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-10-11T20:50:56-04:00</updated><id>http://localhost:4000/</id><title type="html">David Snyder</title><subtitle></subtitle><entry><title type="html">NIST SRE 2016 Xvector Recipe</title><link href="http://localhost:4000/2017/10/04/model_sre16_v2.html" rel="alternate" type="text/html" title="NIST SRE 2016 Xvector Recipe" /><published>2017-10-04T00:14:28-04:00</published><updated>2017-10-04T00:14:28-04:00</updated><id>http://localhost:4000/2017/10/04/model_sre16_v2</id><content type="html" xml:base="http://localhost:4000/2017/10/04/model_sre16_v2.html">&lt;p&gt;The DNN speaker embeddings are now supported in the main branch of &lt;a href=&quot;http://kaldi-asr.org/&quot; target=&quot;_blank&quot;&gt;Kaldi&lt;/a&gt;.
We’ve also added a “bare bones” NIST SRE 2016 recipe to demonstrate the system.
See the &lt;a href=&quot;https://github.com/kaldi-asr/kaldi/pull/1896/&quot; target=&quot;_blank&quot;&gt;pull request&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;The system, built for speaker recognition, consists of a TDNN with a statistics pooling layer.
It’s trained to classify a list of speakers using a multiclass cross entropy objective.
In the future, this will probably be extended to include “same vs different” training.
After training, the last few layers of the network are removed,
and variable-length utterances are mapped to fixed-dimensional embeddings that are used in a PLDA backend (like ivectors).
We’re calling these embeddings “xvectors” in Kaldi speaker recognition recipes.
This is based on &lt;a href=&quot;http://www.danielpovey.com/files/2017_interspeech_embeddings.pdf&quot; target=&quot;_blank&quot;&gt;Deep Neural Network Embeddings for Text-Independent Speaker Verification&lt;/a&gt;
but includes recent enhancements not found in that paper, such as data augmentation.
Look for a more up-to-date paper describing this system in ICASSP 2018.&lt;/p&gt;

&lt;h2 id=&quot;pretrained-model&quot;&gt;Pretrained Model&lt;/h2&gt;
&lt;p&gt;We’ve uploaded a pretrained model on &lt;a href=&quot;http://kaldi-asr.org/models.html&quot; target=&quot;_blank&quot;&gt;kaldi-asr.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The archive 0003_sre16_v2_1a.tar.gz contains files generated from the recipe in egs/sre16/v2/.
It’s contents should be placed in a similar directory, with symbolic links to
sid/, steps/, etc. This was created when the Kaldi master branch was at git
log &lt;a href=&quot;https://github.com/kaldi-asr/kaldi/commit/e082c17d4a8f8a791428ae4d9f7ceb776aef3f0b&quot; target=&quot;_blank&quot;&gt;e082c17d4a8f8a791428ae4d9f7ceb776aef3f0b&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;files-list&quot;&gt;Files list&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; ./
     README.txt               This file
     run.sh                   The recipe that was in egs/sre16/v2/run.sh

 local/nnet3/xvector/tuning/
     run_xvector_1a.sh        Generated the configs, egs, and trained the model

 conf/
     vad.conf                 Energy VAD configration
     mfcc.conf                MFCC configuration

 exp/xvector_nnet_1a/
     final.raw                The pretrained model
     nnet.config              An nnet3 config file for instantiating the model
     extract.config           An nnet3 config file for extracting xvectors
     min_chunk_size           Min chunk size used (see extract_xvectors.sh)
     max_chunk_size           Max chunk size used (see extract_xvectors.sh)
     srand                    The RNG seed used

 exp/xvectors_sre_combined/
     mean.vec                 Vector for centering, from augmented SRE 04-10
     plda                     PLDA model, trained on augmented SRE 04-10
     transform.mat            LDA matrix, trained on augmented SRE 04-10

 exp/xvectors_sre16_major/
     mean.vec                 Vector for centering, from SRE16 major
     plda_adapt               The first PLDA model, adapted to SRE16 major
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h1 id=&quot;training-data&quot;&gt;Training Data&lt;/h1&gt;

&lt;p&gt;The xvector DNN was trained on the following corpora:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     Corpus              LDC Catalog No.
     SWBD2 Phase 1       LDC98S75
     SWBD2 Phase 2       LDC99S79
     SWBD2 Phase 3       LDC2002S06
     SWBD Cellular 1     LDC2001S13
     SWBD Cellular 2     LDC2004S07
     SRE2004             LDC2006S44
     SRE2005 Train       LDC2011S01
     SRE2005 Test        LDC2011S04
     SRE2006 Train       LDC2011S09
     SRE2006 Test 1      LDC2011S10
     SRE2006 Test 2      LDC2012S01
     SRE2008 Train       LDC2011S05
     SRE2008 Test        LDC2011S08
     SRE2010 Eval        LDC2017S06
     Mixer 6             LDC2013S03

 The following datasets were used in data augmentation.

     MUSAN               http://www.openslr.org/17
     RIR_NOISES          http://www.openslr.org/28
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;The models should produce results similar to the following on SRE16.
The acoustic ivector system is included for reference (see egs/sre16/v1).
Note that the PLDA backend used here is still fairly basic.  Results for both
the ivector and xvector systems will improve with more sophisticated adaptation
and score normalization.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  xvector              EER: Pooled 8.57%, Tagalog 12.29%, Cantonese 4.89%
  ivector (from ../v1) EER: Pooled 12.98%, Tagalog 17.8%, Cantonese 8.35%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;citation&quot;&gt;Citation&lt;/h1&gt;

&lt;p&gt;If you want to use the pretrained model in a paper, please cite as:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; @article{snyder2017xvector,
  title={Deep Neural Network Embeddings for Text-Independent Speaker Verification},
  author={Snyder, David and Garcia-Romero, Daniel and Povey, Daniel and Khudanpur, Sanjeev},
  journal={Proc. Interspeech 2017},
  pages={999--1003},
  year={2017}
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">The DNN speaker embeddings are now supported in the main branch of Kaldi. We’ve also added a “bare bones” NIST SRE 2016 recipe to demonstrate the system. See the pull request for more details.</summary></entry></feed>